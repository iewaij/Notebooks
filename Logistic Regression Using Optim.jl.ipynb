{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Optim.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major focus in Machine Learning is classification problem where we want to predict which category a certain object belongs to. For example, we might be interested whether an email is spam or not, which number does a picture show, whether a credit card holder will default or not, etc.\n",
    "\n",
    "For the credit card case, the intuition is that we want to know the probability that card holder $i$ will default, namely $\\pi_{i}$, based on account balance, income, etc. The more certain we are, the closer the output is to 1. If we use Linear Regression, $\\pi_{i}$ may be lager than 1 and is certainly not true. Logistic Regression is thus introduced which has the form\n",
    "\n",
    "$$\n",
    "\\pi_{i} = \\frac{e^{\\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}}}{1+e^{\\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}}}\n",
    "$$\n",
    "\n",
    "The function has an \"S\" shape ranging from 0 to 1 as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 600 400\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip5400\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"600\" height=\"400\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5400)\" points=\"\n",
       "0,400 600,400 600,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5401\">\n",
       "    <rect x=\"120\" y=\"0\" width=\"421\" height=\"400\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5400)\" points=\"\n",
       "32.4745,375.869 580.315,375.869 580.315,11.811 32.4745,11.811 \n",
       "  \" fill=\"#ffffff\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5402\">\n",
       "    <rect x=\"32\" y=\"11\" width=\"549\" height=\"365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip5402)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  83.045,375.869 83.045,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5402)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  195.132,375.869 195.132,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5402)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  307.219,375.869 307.219,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5402)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  419.307,375.869 419.307,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5402)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  531.394,375.869 531.394,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5402)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  32.4745,304.676 580.315,304.676 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5402)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  32.4745,230.758 580.315,230.758 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5402)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  32.4745,156.841 580.315,156.841 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5402)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  32.4745,82.9226 580.315,82.9226 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,375.869 580.315,375.869 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,375.869 32.4745,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  83.045,375.869 83.045,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  195.132,375.869 195.132,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  307.219,375.869 307.219,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  419.307,375.869 419.307,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  531.394,375.869 531.394,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,304.676 40.6921,304.676 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,230.758 40.6921,230.758 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,156.841 40.6921,156.841 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5400)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,82.9226 40.6921,82.9226 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 83.045, 389.669)\" x=\"83.045\" y=\"389.669\">-4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 195.132, 389.669)\" x=\"195.132\" y=\"389.669\">-2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 307.219, 389.669)\" x=\"307.219\" y=\"389.669\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 419.307, 389.669)\" x=\"419.307\" y=\"389.669\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 531.394, 389.669)\" x=\"531.394\" y=\"389.669\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 26.4745, 309.176)\" x=\"26.4745\" y=\"309.176\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 26.4745, 235.258)\" x=\"26.4745\" y=\"235.258\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 26.4745, 161.341)\" x=\"26.4745\" y=\"161.341\">0.6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5400)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 26.4745, 87.4226)\" x=\"26.4745\" y=\"87.4226\">0.8</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip5402)\" style=\"stroke:#009af9; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,375.869 83.0814,371.942 113.494,367.296 139.208,361.031 153.174,356.359 167.14,350.544 180.015,343.978 192.89,336.067 206.694,325.883 220.498,313.745 \n",
       "  236.491,297.053 252.484,277.491 266.697,257.841 280.91,236.396 293.373,216.512 305.837,196.079 333.013,152.009 360.352,112.223 375.465,93.3965 390.577,77.1259 \n",
       "  404.957,64.0038 419.337,53.0401 434.288,43.6967 449.24,36.1703 461.826,31.0318 474.412,26.8147 505.144,19.5108 534.033,15.3518 580.315,11.811 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in case you hadn't installed the package\n",
    "# Pkg.update()\n",
    "# Pkg.add(\"Plots\")\n",
    "using Plots; gr()\n",
    "plot(x->e^x/(1+e^x), legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vector form, the function becomes \n",
    "\n",
    "$$\n",
    "\\pi_{i} = \\frac{e^{\\mathbf{x_i}^T\\beta}}{1+e^{\\mathbf{x_i}^T\\beta}} = \\frac{1}{1+e^{-\\mathbf{x_i}^T\\beta}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{x_i} = \\begin{bmatrix}\n",
    "       1    \\\\\n",
    "       x_{1i} \\\\\n",
    "       x_{2i} \\\\ \n",
    "      \\end{bmatrix}\n",
    "\\quad \\beta = \\begin{bmatrix}\n",
    "               \\beta_0 \\\\\n",
    "               \\beta_1 \\\\\n",
    "               \\beta_2 \\\\ \n",
    "              \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use maximum likelihood estimation to estimate the parameters ($\\beta$), we need find the parameters that maximize the likelihood function given the data we have. Assuming the sample size is $n$ and the data is i.i.d, the likelihood function is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p\\left(y, \\mathbf{X}  \\mid  \\beta \\right) &=\\prod_{i:y=1}\\pi_{i}\\prod_{i:y=o}(1-\\pi_{i})\\\\\n",
    "                                          &=\\prod_{i=1}^{n}\\pi_{i}^{y_{i}}(1-\\pi_{i})^{1-y_{i}}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore the log likelihood function is\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "\\ell(\\beta)&=\\sum_{i=1}^{n}[y_{i}\\log(\\pi_{i})+(1-y_{i})\\log(1-\\pi_{i})]\\\\ \n",
    "& = \\sum_{i=1}^{n}[y_{i}\\mathbf{x_i}^T\\beta - \\log(1+e^{\\mathbf{x_i}^T\\beta})]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Because `Optim.jl` is used to **minimize** a function, the objective function becomes **negative** log likelihood function.\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{minimize}} \\quad f(\\beta) = -\\ell(\\beta) = \\sum_{i=1}^{n}[\\log(1+e^{\\mathbf{x_i}^T\\beta}) - y_{i}\\mathbf{x_i}^T\\beta]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the objective function into code, note that `X` is $n$ rows of $\\mathbf{x_i}^T$ and operators with dot(.) are element-wise.\n",
    "\n",
    "$$\n",
    "\\texttt{X} = \\begin{bmatrix}\n",
    "        1      & x_{11} & x_{21} \\\\\n",
    "        1      & x_{12} & x_{22} \\\\\n",
    "        \\vdots & \\vdots & \\vdots\\\\\n",
    "        1      & x_{1n} & x_{2n} \\\\ \n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\texttt{X*β} = \\begin{bmatrix}\n",
    "        \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{21} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta_0 + \\beta_1 x_{1n} + \\beta_2 x_{2n} \\\\ \n",
    "        \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(β) = sum(log.(1 .+ e.^(X * β)) - y .* (X * β))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Regression\n",
    "Now it's time to perform Logistic Regression on the credit card data from [\"An Introduction to Statistical Learning with Applications in R\"](http://www-bcf.usc.edu/~gareth/ISL/data.html). For this example, we are only interested in the effects of `Balance` as $x_1$ and `Income` as $x_2$ on non-students. Before we start regression, we need prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Default</th><th>Student</th><th>Balance</th><th>Income</th></tr></thead><tbody><tr><th>1</th><td>No</td><td>No</td><td>729.526</td><td>44361.6</td></tr><tr><th>2</th><td>No</td><td>No</td><td>1073.55</td><td>31767.1</td></tr><tr><th>3</th><td>No</td><td>No</td><td>529.251</td><td>35704.5</td></tr><tr><th>4</th><td>No</td><td>No</td><td>785.656</td><td>38463.5</td></tr><tr><th>5</th><td>No</td><td>No</td><td>825.513</td><td>24905.2</td></tr><tr><th>6</th><td>No</td><td>No</td><td>1161.06</td><td>37468.5</td></tr></tbody></table>"
      ],
      "text/plain": [
       "6×4 DataFrames.DataFrame\n",
       "│ Row │ Default │ Student │ Balance │ Income  │\n",
       "├─────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ No      │ No      │ 729.526 │ 44361.6 │\n",
       "│ 2   │ No      │ No      │ 1073.55 │ 31767.1 │\n",
       "│ 3   │ No      │ No      │ 529.251 │ 35704.5 │\n",
       "│ 4   │ No      │ No      │ 785.656 │ 38463.5 │\n",
       "│ 5   │ No      │ No      │ 825.513 │ 24905.2 │\n",
       "│ 6   │ No      │ No      │ 1161.06 │ 37468.5 │"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pkg.add(\"RDatasets\")\n",
    "using RDatasets\n",
    "credit = dataset(\"ISLR\", \"Default\")\n",
    "non_student = credit[credit[:Student] .== \"No\", :] # select non-students\n",
    "head(non_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "outputHidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7056-element Array{Int64,1}:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = hcat(ones(Int8, size(non_student)[1]), non_student[3], non_student[4]) # add 1 as the first column\n",
    "y = Array(recode(non_student[1], \"Yes\"=>1, \"No\"=>0)) # recode categorical data into number 1 or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Free Methods\n",
    "Now we can use `Optim.jl` to estimate $\\beta$. Note that we need pass an array of initial values of `β`. In the example the initial value is `[0.0,0.0,0.0]`. If we don't pass the gradient function, `Optim.jl` will use methods that don't require gradient infomation. The default method is Nelder-Mead, also known as downhill simplex method, which applies a moving simplex to search the designed space untill it reaches a speciﬁed error tolerence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pkg.add(\"Optim\")\n",
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Nelder-Mead\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-10.9369799553686,0.005817604688126401, ...]\n",
       " * Minimum: 5.040173e+02\n",
       " * Iterations: 215\n",
       " * Convergence: true\n",
       "   *  √(Σ(yᵢ-ȳ)²)/n < 1.0e-08: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 389"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, zeros(3)) # equal to optimize(f, zeros(3), NelderMead())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another gradient free method is Simulated Annealing. The main idea is that, instead of searching and staying at local minima, the point has probability $exp(\\frac{-\\Delta f}{kT})$ to move out of it. In our example, the method doesn't provide a good result, the point even stays at the initial value, we will discuss this in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Simulated Annealing\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [0.0,0.0,0.0]\n",
       " * Minimum: 4.890847e+03\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = NaN \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = NaN |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = NaN \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 1001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, zeros(3), SimulatedAnnealing())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask `Optim.jl` to perform a gradient-required method, such as Gradient Descent, without passing the gradient function, `Optim.jl` will provide an approximate gradient and apply the gradient required method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Gradient Descent\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-1.6940577768955823e-6,0.00209726073510965, ...]\n",
       " * Minimum: 8.742771e+02\n",
       " * Iterations: 26\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 3.77e-08 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 1.00e-06 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 1.00e+04 \n",
       "   * Stopped by an increasing objective: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 222\n",
       " * Gradient Calls: 222"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, zeros(3), GradientDescent()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Simulated Annealing Method Fails?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the point didn't move at all using Simulated Annealing method. The reason is that as the point moves around, $-\\Delta f(x)$ is too large that the probability of moving out, $exp(\\frac{-\\Delta f}{kT})$, is nearly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4890.846506030983"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inf"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([0.0, 0.01, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the initial value `[[0.0, 0.0, 0.0]`, the result is around `6931`. If we adjust the parameters a little bit, we will get `Inf`. The point \"moves too much\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optim.jl` uses the function `default_neighbor!` to decide how the point moves. \n",
    "\n",
    "```Julia\n",
    "function default_neighbor!(x::AbstractArray, x_proposal::AbstractArray)\n",
    "    @assert size(x) == size(x_proposal)\n",
    "    for i in 1:length(x)\n",
    "        @inbounds x_proposal[i] = x[i] + randn()\n",
    "    end\n",
    "    return\n",
    "end\n",
    "```\n",
    "\n",
    "As we can see, the point moves `randn()` in each direction, which is too large. We can define a new `neighbor!` function to make the point travel less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neighbor! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function neighbor!(x::AbstractArray, x_proposal::AbstractArray)\n",
    "    @assert size(x) == size(x_proposal)\n",
    "    for i in 1:length(x)\n",
    "        @inbounds x_proposal[i] = x[i] + randn()/10\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Simulated Annealing\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [0.0,0.0,0.0]\n",
       " * Minimum: 4.890847e+03\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = NaN \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = NaN |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = NaN \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 1001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, zeros(3), SimulatedAnnealing(neighbor = neighbor!))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the result is still not ideal, at least it's better than staying at the initial value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Required Methods\n",
    "We can move to gradient required methods by providing gradient information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla f(\\beta) = \\begin{bmatrix}\n",
    "           \\frac{\\partial f}{\\partial \\beta_0}   \\\\\n",
    "           \\frac{\\partial f}{\\partial \\beta_1} \\\\\n",
    "           \\frac{\\partial f}{\\partial \\beta_2} \\\\ \n",
    "           \\end{bmatrix}\n",
    "         = \\begin{bmatrix}\n",
    "           \\sum_{i=1}^{n} e^{\\mathbf{x_i}^T\\beta}/(1+e^{\\mathbf{x_i}^T\\beta}) - y_i \\\\\n",
    "           \\sum_{i=1}^{n} x_{1i}e^{\\mathbf{x_i}^T\\beta}/(1+e^{\\mathbf{x_i}^T\\beta}) - y_ix_{1i}      \\\\\n",
    "           \\sum_{i=1}^{n} x_{2i}e^{\\mathbf{x_i}^T\\beta}/(1+e^{\\mathbf{x_i}^T\\beta}) - y_ix_{2i}      \\\\ \n",
    "           \\end{bmatrix}\n",
    "         = \\begin{bmatrix}\n",
    "           \\sum_{i=1}^{n} (1+e^{-\\mathbf{x_i}^T\\beta})^{-1} - y_i \\\\\n",
    "           \\sum_{i=1}^{n} x_{1i}(1+e^{-\\mathbf{x_i}^T\\beta})^{-1} - y_ix_{1i}      \\\\\n",
    "           \\sum_{i=1}^{n} x_{2i}(1+e^{-\\mathbf{x_i}^T\\beta})^{-1} - y_ix_{2i}      \\\\ \n",
    "           \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put exclamation mark(!) at the end of the function name because we make modifications on the `storage` vector variable, or you can call it `gradient` variable. After each iteration, the `storage` variable will change as the value of `β` changes, we only need transform each entry of $\\nabla f(\\beta)$ into code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g! (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function g!(storage, β)\n",
    "    storage[1]= sum((1 .+ e.^(-X * β)).^-1 - y)\n",
    "    storage[2]= sum(X[:,2]./(1 .+ e.^(-X * β)) - y.*X[:,2])\n",
    "    storage[3]= sum(X[:,3]./(1 .+ e.^(-X * β)) - y.*X[:,3])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default method is L-BFGS which is limited memory version of BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: L-BFGS\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-10.936996147926969,0.00581761412461086, ...]\n",
       " * Minimum: 5.040173e+02\n",
       " * Iterations: 23\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 2.85e-10 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 1.13e-16 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: true \n",
       "     |g(x)| = 2.33e-10 \n",
       "   * Stopped by an increasing objective: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 148\n",
       " * Gradient Calls: 148"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, zeros(3))  # equal to optimize(f, g!, zeros(3), LBFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BFGS method uses gradient to approxiamate the Hessian and apply Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: BFGS\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-10.936996147644159,0.005817614124717566, ...]\n",
       " * Minimum: 5.040173e+02\n",
       " * Iterations: 16\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 2.32e-12 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 1.13e-16 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: true \n",
       "     |g(x)| = 1.28e-09 \n",
       "   * Stopped by an increasing objective: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 116\n",
       " * Gradient Calls: 116"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, zeros(3), BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optim.jl` can also apply Gradient Descent method and Conjugate Gradient Descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Gradient Descent\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-1.1722447127568948e-6,0.0015699471334478488, ...]\n",
       " * Minimum: 8.868502e+02\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 3.38e-07 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 1.46e-05 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 4.08e+04 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 2616\n",
       " * Gradient Calls: 2616"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, zeros(3), GradientDescent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Conjugate Gradient\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-11.565274126641834,0.006073747862744703, ...]\n",
       " * Minimum: 5.045925e+02\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 6.55e-06 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 5.50e-10 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 1.04e+02 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 2135\n",
       " * Gradient Calls: 1373"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, zeros(3), ConjugateGradient())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result tells us that the algorithm stops because it reached maximum number of iterations (1000), we can adjust the value of iterations by [changing configurable options](http://julianlsolvers.github.io/Optim.jl/stable/user/config/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Gradient Descent\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-1.049531296428253e-5,0.0022618381882457805, ...]\n",
       " * Minimum: 8.735567e+02\n",
       " * Iterations: 10000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 8.74e-10 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 9.96e-11 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 1.02e+02 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 25179\n",
       " * Gradient Calls: 25179"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, zeros(3), GradientDescent(), Optim.Options(iterations = 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian Required Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optim.jl` can apply Newton's method if we provide the Hessian. This method is regarded as the gold standard of unconstrained smooth optimization, though it could be painful to calculate the Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{H}f(\\beta) = \\begin{bmatrix}\n",
    "              \\frac{\\partial}{\\partial \\beta_0}   \\\\\n",
    "              \\frac{\\partial}{\\partial \\beta_1} \\\\\n",
    "              \\frac{\\partial}{\\partial \\beta_2} \\\\ \n",
    "              \\end{bmatrix}\n",
    "              \\begin{bmatrix} \n",
    "              \\frac{\\partial f}{\\partial \\beta_0} \\ \\frac{\\partial f}{\\partial \\beta_1} \\ \\frac{\\partial f}{\\partial \\beta_2}\n",
    "              \\end{bmatrix}\n",
    "            = \\begin{bmatrix}\n",
    "              \\frac{\\partial^2 f}{\\partial \\beta_0^2} \\ \\frac{\\partial^2 f}{\\partial \\beta_0\\beta_1} \\ \\frac{\\partial^2 f}{\\partial \\beta_0\\beta_2}\\\\\n",
    "              \\frac{\\partial^2 f}{\\partial \\beta_0\\beta_1} \\ \\frac{\\partial^2 f}{\\partial \\beta_1^2} \\ \\frac{\\partial^2 f}{\\partial \\beta_1\\beta_2}\\\\\n",
    "              \\frac{\\partial^2 f}{\\partial \\beta_0\\beta_2} \\ \\frac{\\partial^2 f}{\\partial \\beta_1\\beta_2} \\ \\frac{\\partial^2 f}{\\partial \\beta_2^2} \\\\ \n",
    "              \\end{bmatrix}\n",
    "            = \\begin{bmatrix}\n",
    "              \\sum_{i=1}^{n}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2} & \n",
    "              \\sum_{i=1}^{n}x_{1i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2} & \n",
    "              \\sum_{i=1}^{n}x_{2i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2} \\\\\n",
    "              \\sum_{i=1}^{n}x_{1i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}         & \n",
    "              \\sum_{i=1}^{n}x_{1i}^2e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}       &\n",
    "              \\sum_{i=1}^{n}x_{1i}x_{2i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}   \\\\\n",
    "              \\sum_{i=1}^{n}x_{2i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}         &\n",
    "              \\sum_{i=1}^{n}x_{1i}x_{2i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}   & \n",
    "              \\sum_{i=1}^{n}x_{2i}^2e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}       \\\\ \n",
    "              \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h! (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function h!(storage, β)\n",
    "    storage[1,1]= sum(e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "    storage[1,2]= sum(X[:,2] .* e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "    storage[1,3]= sum(X[:,3] .* e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "    storage[2,2]= sum(X[:,2].^2 .* e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "    storage[2,3]= sum(X[:,2] .* X[:,3] .* e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "    storage[3,3]= sum(X[:,3].^2 .* e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Newton's Method\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-10.936995666059834,0.005817613899214999, ...]\n",
       " * Minimum: 5.040173e+02\n",
       " * Iterations: 290\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 1.41e-07 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: true\n",
       "     |f(x) - f(x')| = 0.00e+00 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 2.20e-01 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 844\n",
       " * Gradient Calls: 844\n",
       " * Hessian Calls: 290"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, h!, zeros(3)) # equal to optimize(f, g!, h!, zeros(3), Newton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we put some restrictions on Newton's Method, such method is called Newton's Method With a Trust Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Newton's Method (Trust Region)\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-9.20575398819707,0.005142584666976512, ...]\n",
       " * Minimum: 5.090461e+02\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 2.48e-03 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 2.98e-05 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 6.12e+00 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 1001\n",
       " * Gradient Calls: 1001\n",
       " * Hessian Calls: 1001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, h!, zeros(3), NewtonTrustRegion())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Results\n",
    "After applying the optimization algorithm, results($\\beta$) can be retrived by the following steps. Since the functions are not exported, they have to be prefixed by \"`Optim.`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: L-BFGS\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-10.936996147926969,0.00581761412461086, ...]\n",
       " * Minimum: 5.040173e+02\n",
       " * Iterations: 23\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 2.85e-10 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 1.13e-16 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: true \n",
       "     |g(x)| = 2.33e-10 \n",
       "   * Stopped by an increasing objective: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 148\n",
       " * Gradient Calls: 148"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = optimize(f, g!, zeros(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " -10.937     \n",
       "   0.00581761\n",
       "   1.59712e-6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_β = Optim.minimizer(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the results with `GLM.jl` which is dedicated to performe Logistic Regression and other linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatsModels.DataFrameRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Array{Float64,1},Distributions.Binomial{Float64},GLM.LogitLink},GLM.DensePredChol{Float64,Base.LinAlg.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}\n",
       "\n",
       "Formula: Default ~ 1 + Balance + Income\n",
       "\n",
       "Coefficients:\n",
       "               Estimate   Std.Error  z value Pr(>|z|)\n",
       "(Intercept)     -10.937    0.575173 -19.0151   <1e-79\n",
       "Balance      0.00581761 0.000293772  19.8031   <1e-86\n",
       "Income       1.59712e-6  8.66998e-6 0.184212   0.8538\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pkg.add(\"GLM\")\n",
    "using GLM\n",
    "lr = glm(@formula(Default ~ Balance + Income), non_student, Binomial(), LogitLink())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both packages (`Optim.jl` and `GLM.jl`) provide same estimates. From the results above, we can conclude that \"Balance\" is an important factor to predict whether a non-student person will default or not, while \"Income\" is not statistically significant."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "julia-0.6"
  },
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
