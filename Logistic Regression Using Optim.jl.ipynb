{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Optim.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major focus in Machine Learning is classification problem where we want to predict which category a certain object belongs to. For example, we might be interested whether an email is spam or not, which number does a picture show, whether a credit card holder will default or not, etc.\n",
    "\n",
    "For the credit card case, the intuition is that we want to know the probability that card holder $i$ will default, namely $\\pi_{i}$, based on account balance, income, etc. The more certain we are, the closer the output is to 1. If we use Linear Regression, $\\pi_{i}$ may be lager than 1, which is certainly not true. Logistic Regression is introduced to solve such issue which has the form\n",
    "\n",
    "$$\n",
    "\\pi_{i} = \\frac{e^{\\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}}}{1+e^{\\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}}}\n",
    "$$\n",
    "\n",
    "The function has an \"S\" shape ranging from 0 to 1 as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 600 400\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip0000\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"600\" height=\"400\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip0000)\" points=\"\n",
       "0,400 600,400 600,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0001\">\n",
       "    <rect x=\"120\" y=\"0\" width=\"421\" height=\"400\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip0000)\" points=\"\n",
       "32.4745,375.869 580.315,375.869 580.315,11.811 32.4745,11.811 \n",
       "  \" fill=\"#ffffff\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0002\">\n",
       "    <rect x=\"32\" y=\"11\" width=\"549\" height=\"365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip0002)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  83.045,375.869 83.045,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0002)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  195.132,375.869 195.132,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0002)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  307.219,375.869 307.219,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0002)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  419.307,375.869 419.307,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0002)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  531.394,375.869 531.394,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0002)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  32.4745,304.676 580.315,304.676 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0002)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  32.4745,230.758 580.315,230.758 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0002)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  32.4745,156.841 580.315,156.841 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0002)\" style=\"stroke:#000000; stroke-width:0.5; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  32.4745,82.9226 580.315,82.9226 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,375.869 580.315,375.869 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,375.869 32.4745,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  83.045,375.869 83.045,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  195.132,375.869 195.132,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  307.219,375.869 307.219,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  419.307,375.869 419.307,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  531.394,375.869 531.394,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,304.676 40.6921,304.676 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,230.758 40.6921,230.758 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,156.841 40.6921,156.841 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0000)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,82.9226 40.6921,82.9226 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 83.045, 389.669)\" x=\"83.045\" y=\"389.669\">-4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 195.132, 389.669)\" x=\"195.132\" y=\"389.669\">-2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 307.219, 389.669)\" x=\"307.219\" y=\"389.669\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 419.307, 389.669)\" x=\"419.307\" y=\"389.669\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 531.394, 389.669)\" x=\"531.394\" y=\"389.669\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 26.4745, 309.176)\" x=\"26.4745\" y=\"309.176\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 26.4745, 235.258)\" x=\"26.4745\" y=\"235.258\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 26.4745, 161.341)\" x=\"26.4745\" y=\"161.341\">0.6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0000)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 26.4745, 87.4226)\" x=\"26.4745\" y=\"87.4226\">0.8</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip0002)\" style=\"stroke:#009af9; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  32.4745,375.869 83.0814,371.942 113.494,367.296 139.208,361.031 153.174,356.359 167.14,350.544 180.015,343.978 192.89,336.067 206.694,325.883 220.498,313.745 \n",
       "  236.491,297.053 252.484,277.491 266.697,257.841 280.91,236.396 293.373,216.512 305.837,196.079 333.013,152.009 360.352,112.223 375.465,93.3965 390.577,77.1259 \n",
       "  404.957,64.0038 419.337,53.0401 434.288,43.6967 449.24,36.1703 461.826,31.0318 474.412,26.8147 505.144,19.5108 534.033,15.3518 580.315,11.811 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in case you hadn't installed the package\n",
    "# Pkg.update()\n",
    "# Pkg.add(\"Plots\")\n",
    "using Plots; gr()\n",
    "plot(x->e^x/(1+e^x), legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vector form, the function becomes \n",
    "\n",
    "$$\n",
    "\\pi_{i} = \\frac{e^{\\mathbf{x_i}^T\\beta}}{1+e^{\\mathbf{x_i}^T\\beta}} = \\frac{1}{1+e^{-\\mathbf{x_i}^T\\beta}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{x_i} = \\begin{bmatrix}\n",
    "       1    \\\\\n",
    "       x_{1i} \\\\\n",
    "       x_{2i} \\\\ \n",
    "      \\end{bmatrix}\n",
    "\\quad \\beta = \\begin{bmatrix}\n",
    "               \\beta_0 \\\\\n",
    "               \\beta_1 \\\\\n",
    "               \\beta_2 \\\\ \n",
    "              \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use maximum likelihood estimation to estimate the parameters ($\\beta$), we need find the parameters that maximize the likelihood function given the data we have. Assuming the sample size is $n$ and the data is i.i.d, the likelihood function is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p\\left(y, \\mathbf{X}  \\mid  \\beta \\right) &=\\prod_{i:y=1}\\pi_{i}\\prod_{i:y=o}(1-\\pi_{i})\\\\\n",
    "                                          &=\\prod_{i=1}^{n}\\pi_{i}^{y_{i}}(1-\\pi_{i})^{1-y_{i}}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore the log likelihood function is\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "\\ell(\\beta)&=\\sum_{i=1}^{n}[y_{i}\\log(\\pi_{i})+(1-y_{i})\\log(1-\\pi_{i})]\\\\ \n",
    "& = \\sum_{i=1}^{n}[y_{i}\\mathbf{x_i}^T\\beta - \\log(1+e^{\\mathbf{x_i}^T\\beta})]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Because `Optim.jl` is used to **minimize** a function, the objective function becomes **negative** log likelihood function.\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{minimize}} \\quad f(\\beta) = -\\ell(\\beta) = \\sum_{i=1}^{n}[\\log(1+e^{\\mathbf{x_i}^T\\beta}) - y_{i}\\mathbf{x_i}^T\\beta]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the objective function into code, note that `X` is $n$ rows of $\\mathbf{x_i}^T$ and operators with dot(.) are element-wise.\n",
    "\n",
    "$$\n",
    "\\texttt{X} = \\begin{bmatrix}\n",
    "        1      & x_{11} & x_{21} \\\\\n",
    "        1      & x_{12} & x_{22} \\\\\n",
    "        \\vdots & \\vdots & \\vdots\\\\\n",
    "        1      & x_{1n} & x_{2n} \\\\ \n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\texttt{X*β} = \\begin{bmatrix}\n",
    "        \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{21} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta_0 + \\beta_1 x_{1n} + \\beta_2 x_{2n} \\\\ \n",
    "        \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(β) = sum(log.(1 .+ e.^(X * β)) - y .* (X * β))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Regression\n",
    "We are now performing Logistic Regression on the credit card data from [\"An Introduction to Statistical Learning with Applications in R\"](http://www-bcf.usc.edu/~gareth/ISL/data.html). For this example, we are only interested in the effects of `Balance` as $x_1$ and `Income` as $x_2$. First let's prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Default</th><th>Student</th><th>Balance</th><th>Income</th></tr></thead><tbody><tr><th>1</th><td>0</td><td>No</td><td>729.526</td><td>44361.6</td></tr><tr><th>2</th><td>0</td><td>Yes</td><td>817.18</td><td>12106.1</td></tr><tr><th>3</th><td>0</td><td>No</td><td>1073.55</td><td>31767.1</td></tr><tr><th>4</th><td>0</td><td>No</td><td>529.251</td><td>35704.5</td></tr><tr><th>5</th><td>0</td><td>No</td><td>785.656</td><td>38463.5</td></tr><tr><th>6</th><td>0</td><td>Yes</td><td>919.589</td><td>7491.56</td></tr></tbody></table>"
      ],
      "text/plain": [
       "6×4 DataFrames.DataFrame\n",
       "│ Row │ Default │ Student │ Balance │ Income  │\n",
       "├─────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1   │ 0       │ No      │ 729.526 │ 44361.6 │\n",
       "│ 2   │ 0       │ Yes     │ 817.18  │ 12106.1 │\n",
       "│ 3   │ 0       │ No      │ 1073.55 │ 31767.1 │\n",
       "│ 4   │ 0       │ No      │ 529.251 │ 35704.5 │\n",
       "│ 5   │ 0       │ No      │ 785.656 │ 38463.5 │\n",
       "│ 6   │ 0       │ Yes     │ 919.589 │ 7491.56 │"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pkg.add(\"RDatasets\")\n",
    "using RDatasets\n",
    "credit = dataset(\"ISLR\", \"Default\")\n",
    "credit[1] = recode(credit[1], \"Yes\"=>1, \"No\"=>0) # recode categorical data into number 1 or 0\n",
    "head(credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "outputHidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Array{Int64,1}:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = hcat(ones(Int8, size(credit)[1]), credit[3], credit[4]) # add 1 as the first column\n",
    "y = Array(credit[1]) # convert into Array type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Free Methods\n",
    "Now we can use `Optim.jl` to estimate $\\beta$. Note that we need pass an array of initial values of `β`. In the example the initial value is `[0.0,0.0,0.0]`. If we don't pass the gradient function, `Optim.jl` will use methods that don't require gradient infomation. The default method is Nelder-Mead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pkg.add(\"Optim\")\n",
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Nelder-Mead\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-11.540474285836979,0.005647107711267278, ...]\n",
       " * Minimum: 7.894831e+02\n",
       " * Iterations: 178\n",
       " * Convergence: true\n",
       "   *  √(Σ(yᵢ-ȳ)²)/n < 1.0e-08: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 327"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, zeros(3)) # equal to optimize(f, zeros(3), NelderMead())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another gradient free method is Simulated Annealing, though it doesn't seem to work well in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Simulated Annealing\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [0.0,0.0,0.0]\n",
       " * Minimum: 6.931472e+03\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = NaN \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = NaN |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = NaN \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 1001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, zeros(3), SimulatedAnnealing())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no gradient function is passed, `Optim.jl` will provide an approximate gradient and apply gradient required method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: L-BFGS\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-11.495346939483449,0.005638072208404706, ...]\n",
       " * Minimum: 7.895012e+02\n",
       " * Iterations: 20\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 2.12e-07 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 1.91e-10 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 2.50e-03 \n",
       "   * Stopped by an increasing objective: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 144\n",
       " * Gradient Calls: 144"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, zeros(3), LBFGS()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Required methods\n",
    "We can move to gradient required methods by providing gradient information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla f(\\beta) = \\begin{bmatrix}\n",
    "           \\frac{\\partial f}{\\partial \\beta_0}   \\\\\n",
    "           \\frac{\\partial f}{\\partial \\beta_1} \\\\\n",
    "           \\frac{\\partial f}{\\partial \\beta_2} \\\\ \n",
    "           \\end{bmatrix}\n",
    "         = \\begin{bmatrix}\n",
    "           \\sum_{i=1}^{n} e^{\\mathbf{x_i}^T\\beta}/(1+e^{\\mathbf{x_i}^T\\beta}) - y_i \\\\\n",
    "           \\sum_{i=1}^{n} x_{1i}e^{\\mathbf{x_i}^T\\beta}/(1+e^{\\mathbf{x_i}^T\\beta}) - y_ix_{1i}      \\\\\n",
    "           \\sum_{i=1}^{n} x_{2i}e^{\\mathbf{x_i}^T\\beta}/(1+e^{\\mathbf{x_i}^T\\beta}) - y_ix_{2i}      \\\\ \n",
    "           \\end{bmatrix}\n",
    "         = \\begin{bmatrix}\n",
    "           \\sum_{i=1}^{n} (1+e^{-\\mathbf{x_i}^T\\beta})^{-1} - y_i \\\\\n",
    "           \\sum_{i=1}^{n} x_{1i}(1+e^{-\\mathbf{x_i}^T\\beta})^{-1} - y_ix_{1i}      \\\\\n",
    "           \\sum_{i=1}^{n} x_{2i}(1+e^{-\\mathbf{x_i}^T\\beta})^{-1} - y_ix_{2i}      \\\\ \n",
    "           \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put exclamation mark(!) at the end of the function name because we make modifications on the `storage` vector variable, or you can call it `gradient` variable. After each iteration, the `storage` variable will change as the value of `β` changes, we only need transform each entry of $\\nabla f(\\beta)$ into code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function g!(storage, β)\n",
    "    storage[1]= sum((1 .+ e.^(-X * β)).^-1 - y)\n",
    "    storage[2]= sum(X[:,2]./(1 .+ e.^(-X * β)) - y.*X[:,2])\n",
    "    storage[3]= sum(X[:,3]./(1 .+ e.^(-X * β)) - y.*X[:,3])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default method is L-BFGS which is limited memory version of BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: L-BFGS\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-11.540468449934565,0.005647102950316482, ...]\n",
       " * Minimum: 7.894831e+02\n",
       " * Iterations: 20\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 1.45e-10 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: true\n",
       "     |f(x) - f(x')| = 0.00e+00 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 1.43e-08 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 145\n",
       " * Gradient Calls: 145"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, zeros(3))  # equal to optimize(f, g!, zeros(3), LBFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BFGS method uses gradient to approxiamate the Hessian and apply Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: BFGS\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-11.540468419741224,0.005647102938940267, ...]\n",
       " * Minimum: 7.894831e+02\n",
       " * Iterations: 15\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 3.02e-08 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 1.44e-16 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 6.44e-06 \n",
       "   * Stopped by an increasing objective: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 114\n",
       " * Gradient Calls: 114"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, zeros(3), BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optim.jl` can also apply Gradient Descent method and Conjugate Gradient Descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Gradient Descent\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-4.491384964357726e-6,0.00040759649811412105, ...]\n",
       " * Minimum: 1.734554e+03\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 2.57e-09 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 4.08e-10 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 2.39e+02 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 2587\n",
       " * Gradient Calls: 2587"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, zeros(3), GradientDescent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Conjugate Gradient\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-12.38365166873607,0.006045298971043064, ...]\n",
       " * Minimum: 7.912599e+02\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 1.28e-06 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 8.44e-09 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 8.82e+02 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 2137\n",
       " * Gradient Calls: 1247"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, zeros(3), ConjugateGradient())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian Required Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optim.jl` can apply Newton's method if we provide the Hessian. This method is the gold standard of unconstrained smooth optimization, though it could be painful to calculate the Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{H}f(\\beta) = \\begin{bmatrix}\n",
    "              \\frac{\\partial}{\\partial \\beta_0}   \\\\\n",
    "              \\frac{\\partial}{\\partial \\beta_1} \\\\\n",
    "              \\frac{\\partial}{\\partial \\beta_2} \\\\ \n",
    "              \\end{bmatrix}\n",
    "              \\begin{bmatrix} \n",
    "              \\frac{\\partial f}{\\partial \\beta_0} \\ \\frac{\\partial f}{\\partial \\beta_1} \\ \\frac{\\partial f}{\\partial \\beta_2}\n",
    "              \\end{bmatrix}\n",
    "            = \\begin{bmatrix}\n",
    "              \\frac{\\partial^2 f}{\\partial \\beta_0^2} \\ \\frac{\\partial^2 f}{\\partial \\beta_0\\beta_1} \\ \\frac{\\partial^2 f}{\\partial \\beta_0\\beta_2}\\\\\n",
    "              \\frac{\\partial^2 f}{\\partial \\beta_0\\beta_1} \\ \\frac{\\partial^2 f}{\\partial \\beta_1^2} \\ \\frac{\\partial^2 f}{\\partial \\beta_1\\beta_2}\\\\\n",
    "              \\frac{\\partial^2 f}{\\partial \\beta_0\\beta_2} \\ \\frac{\\partial^2 f}{\\partial \\beta_1\\beta_2} \\ \\frac{\\partial^2 f}{\\partial \\beta_2^2} \\\\ \n",
    "              \\end{bmatrix}\n",
    "            = \\begin{bmatrix}\n",
    "              \\sum_{i=1}^{n}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2} & \n",
    "              \\sum_{i=1}^{n}x_{1i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2} & \n",
    "              \\sum_{i=1}^{n}x_{2i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2} \\\\\n",
    "              \\sum_{i=1}^{n}x_{1i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}         & \n",
    "              \\sum_{i=1}^{n}x_{1i}^2e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}       &\n",
    "              \\sum_{i=1}^{n}x_{1i}x_{2i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}   \\\\\n",
    "              \\sum_{i=1}^{n}x_{2i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}         &\n",
    "              \\sum_{i=1}^{n}x_{1i}x_{2i}e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}   & \n",
    "              \\sum_{i=1}^{n}x_{2i}^2e^{-\\mathbf{x_i}^T\\beta}(1+e^{-\\mathbf{x_i}^T\\beta})^{-2}       \\\\ \n",
    "              \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h! (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function h!(storage, β)\n",
    "    storage[1,1]= sum(e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "    storage[1,2]= sum(X[:,2] .* e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "    storage[1,3]= sum(X[:,3] .* e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "    storage[2,2]= sum(X[:,2].^2 .* e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "    storage[2,3]= sum(X[:,2] .* X[:,3] .* e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "    storage[3,3]= sum(X[:,3].^2 .* e.^(-X * β) .* (1 .+ e.^(-X * β)).^-2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Newton's Method\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-11.540468994905527,0.005647103239234815, ...]\n",
       " * Minimum: 7.894831e+02\n",
       " * Iterations: 274\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 8.42e-08 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: true\n",
       "     |f(x) - f(x')| = 0.00e+00 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 2.37e-01 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 789\n",
       " * Gradient Calls: 789\n",
       " * Hessian Calls: 274"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, h!, zeros(3)) # equal to optimize(f, g!, h!, zeros(3), Newton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we put some restrictions on Newton's Method, such method is called Newton's Method With a Trust Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Newton's Method (Trust Region)\n",
       " * Starting Point: [0.0,0.0,0.0]\n",
       " * Minimizer: [-11.54046844993458,0.0056471029503164915, ...]\n",
       " * Minimum: 7.894831e+02\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| ≤ 1.0e-32: false \n",
       "     |x - x'| = 6.78e-21 \n",
       "   * |f(x) - f(x')| ≤ 1.0e-32 |f(x)|: false\n",
       "     |f(x) - f(x')| = 0.00e+00 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 2.04e-09 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 1001\n",
       " * Gradient Calls: 1001\n",
       " * Hessian Calls: 1001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, h!, zeros(3), NewtonTrustRegion())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Results\n",
    "After applying the optimization algorithm, we can retrieve our results($\\beta$) by following steps. Since some functions are not exported, they have to be prefixed by `Optim.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " -11.5405   \n",
       "   0.0056471\n",
       "   2.0809e-5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = optimize(f, g!, h!, zeros(3))\n",
    "estimate_β = Optim.minimizer(results)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "julia-0.6"
  },
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
